<!DOCTYPE html>
<html lang="en-gb">

  <head>
    <meta charset="utf-8">
    <meta name="generator" content="Hugo 0.37.1" />

    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="">
    <meta property="og:url" content="https://matmoore.github.io/accelerator/post/dynamic-bayesian-network-model/">

    <title>Predicting search result relevance with a click model - Accelerator project journal</title>
    <meta property="og:title" content="Predicting search result relevance with a click model - Accelerator project journal">
    <meta property="og:type" content="article">
    <meta name="description" content="">

    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Code+Pro|Arvo:400,700">
    <link rel="stylesheet" href="../../css/highlight.css">
    <link rel="stylesheet" href="../../css/journal.css">
    <link href="../../index.xml" rel="alternate" type="application/rss+xml" title="Accelerator project journal">

  </head>

  <body>
    <div class="container">

      <nav class="site-nav">
        <a href="https://matmoore.github.io/accelerator/">Index</a>
      </nav>

  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">Predicting search result relevance with a click model <span class="tag">Week 3</span></h1>
      <time class="post-date" datetime="2018-04-08 10:00:00 UTC">08 Apr 2018</time>

    </header>
    <p>For my project I want to use search logs to model user behaviour on GOV.UK search.</p>

<p>The approach I want to follow is based on a paper titled
<a href="http://olivier.chapelle.cc/pub/DBN_www2009.pdf">A Dynamic Bayesian Network Click Model for Web Search Ranking</a> (Chapelle and Zhang 2009)</p>

<h2 id="overview-of-the-paper">Overview of the paper</h2>

<p>The Dynamic Bayesian Network model is one of many <a href="https://clickmodels.weebly.com/about.html">click models</a> for modelling search behaviour in terms of random variables. Given a search engine result page, a click model should be able to predict the probability that a user will click on any of the links on the page.</p>

<p>The paper starts by introducing the Dynamic Bayesian Network (DBN) model in the context of earlier click models. It then discusses some experiments based on web search data:</p>

<ul>
<li>Section 5 describes how to parameterise the model in the simplest case (for the full thing they use Expectation Maximization (EM), described in the appendix)</li>
<li>Section 4.1 predicts the click through rate of position 1 links, to test that the model describes the data</li>
<li>Section 4.2 predicts relevance of observed (query, url) pairs and compares it to editorial judgements to show that the model is almost as good as getting humans to judge relevance</li>
<li>Section 4.3 uses the predicted relevance to train a learning to rank (LTR) model, so that they can rank results based on a feature set instead of needing a complete set of relevance estimations for every (url, query) pair at runtime</li>
</ul>

<h2 id="what-is-a-dbn">What is a DBN?</h2>

<p>A dynamic bayesian network is a type of Probabalistic Graphical Model (PGM). This is basically a graph that shows how random variables depend on each other. Random variables can be things you observe, or things that are hidden (latent variables). In this particular case most of the variables in the model are hidden variables.</p>

<p>I've found the <a href="https://www.coursera.org/learn/probabilistic-graphical-models">Probabalistic Graphic Models</a> course on Coursera useful for understanding this stuff (but I haven't completed it yet).</p>

<p>The &quot;dynamic&quot; part basically means that you are modelling some kind of sequence. In this case a user is examining a list of search results, and the probabality of clicking on any particular link is dependent on the probability of clicking the previous one.</p>

<p>The authors note that this is similar to a Hidden Markov model (HMM). <a href="https://datascience.stackexchange.com/questions/10000/what-is-the-difference-between-a-dynamic-bayes-network-and-a-hmm">HMM is a special case of DBN</a>.</p>

<h3 id="what-the-dbn-looks-like">What the DBN looks like</h3>

<p>The model presented in the paper looks like this:
<figure><img src="../../images/dbn.png" alt="diagram of the DBN click model"></figure></p>

<p>This says that the probability of a search result being clicked depends on two things: whether the result is &quot;attractive&quot;, <span  class="math">\(A\)</span>, and whether the user actually bothered looking at that result, <span  class="math">\(E\)</span>. Whether the user looks at the next result depends on whether they were satisfied with the result they just clicked on, <span  class="math">\(S\)</span>.</p>

<p>The box around the variables means that everything inside the box applies to the whole sequence. So for the first 3 search results, the graph expands to this:</p>

<p><figure><img src="../../images/dbn-expanded.png" alt="expanded diagram of the DBN click model"></figure></p>

<p>Note that the first result is a special case, because <span  class="math">\(E_1\)</span> doesn't depend on anything else. The user always examines the first result. The probability they click on it just depends on how attractive it is.</p>

<h3 id="assumptions-of-the-model">Assumptions of the model</h3>

<p>These are the basic assumptions of them model, which allow us to define conditional probability distributions for each of the variables.</p>

<ol>
<li>Links are clicked if and only if the user examines them and the URL is attractive.</li>
<li>Attractiveness is an inherent quality of the URL, parameterized by the <span  class="math">\(a_u\)</span> variable</li>
<li>After a clicking a link the user may or may not be satisfied. This is also an inherent quality of the URL, parameterised by <span  class="math">\(s_u\)</span></li>
<li>Users stop examining new results once satisfied with a result they clicked on</li>
<li>There is a probability <span  class="math">\(1-\gamma\)</span> of abandoning a search after clicking a result and not being satisfied. The simplified model ignores this.</li>
<li>Users examine all links in order and don’t skip over a link without examining it.

<ul>
<li>In section 4 they say 3% of sessions contain out of order clicks, which goes against this assumption. They discarded the data but said they could also just ignore the ordering here (i.e. treat the sessions as if the user clicked the links in the ranked order). I think this is what I'm going to do initially, because it makes it easier to get the data I need.</li>
</ul></li>
</ol>

<h2 id="extracting-relevance-from-the-trained-model">Extracting relevance from the trained model</h2>

<p>The DBN model has 3 sets of parameters:</p>

<ul>
<li>A single variable <span  class="math">\(\gamma\)</span>, which is a measure of how determined users are to find what they're looking for. In the simplified model I'm going to start with, <span  class="math">\(\gamma = 1\)</span>, meaning users are assumed to examine everything on the page until they find a relevant result.</li>
<li><span  class="math">\(a_u\)</span>, defined for every (query, result) pair. This is the probability the result looks relevant for that query.</li>
<li><span  class="math">\(s_u\)</span>, defined for every (query, result) pair. This is the probability the result actually satisfies the user.</li>
</ul>

<p>They define relevance as the product of <span  class="math">\(a_u\)</span> and <span  class="math">\(s_u\)</span>, so once you've trained the model you know what's relevant and what isn't, and you can generate an optimal ranking that puts the most relevant stuff on the top.</p>

<p>This can then be used as training data to learn a ranking function that can handle any query (section 4.3). The authors found this worked almost as well as using editorial relevance judgements.</p>

<h2 id="training-the-simplified-model">Training the simplified model</h2>

<p>For the simplified model where <span  class="math">\(\gamma=1\)</span>, the algorithm for optimising the parameters is basically just counting things.</p>

<pre><code class="language-python">a = {}
s = {}
for u in urls:
  a[u] = Count(numerator=0, denominator=0)
  s[u] = Count(numerator=0, denominator=0)

for session in sessions:
  # Everything above and including the last clicked URL
  for u in session.examined_urls:
    a[u].denominator += 1

  for u in session.clicked_urls:
    a[u].numerator += 1
    s[u].denominator += 1

  s[session.final_clicked_url].numerator += 1
</code></pre>

<p>The calculation of <span  class="math">\(a_u\)</span> is similar to the overall clickthrough rate, but it takes into account what the user is likely to have seen, rather than counting everything displayed on the page.</p>

<p><span  class="math">\(s_u\)</span> is just the percentage of clicks where the user doesn't try another result afterwards.</p>

<p>The authors took some steps to filter the data before splitting it into training and test sets:</p>

<ul>
<li>Ignore anything that's not on the first page of search results (in their case this is 10 links, for GOV.UK it’s 20 links)</li>
<li>Discard all queries with &lt; 10 sessions</li>
<li>Discard sessions with out of order clicks (see above)</li>
</ul>

<h3 id="beta-priors">Beta priors</h3>

<p>In the paper they include a <a href="https://en.wikipedia.org/wiki/Beta_distribution">beta prior</a> for both a and s. So the actual values of <span  class="math">\(a_u\)</span> and <span  class="math">\(s_u\)</span> are:</p>

<pre><code class="language-python">a_u = (a[u].numerator + alpha_a) / (a[u].denominator + alpha_a + beta_a)
s_u = (s[u].numerator + alpha_s) / (s[u].denominator + alpha_s + beta_s)
</code></pre>

<p>I don’t really understand this at the moment, but presumably I'm supposed to have some prior knowledge about <span  class="math">\(a_u\)</span> and <span  class="math">\(s_u\)</span>.</p>

<p>In theory I do have some additional data about GOV.UK pages I could use, but it's independent of what the user was looking for, so I don't know if it's applicable here.</p>


  </article>


      <footer class="site-footer">
        <span itemscope itemtype="http://schema.org/Person">
          <link itemprop="url" href="https://matmoore.github.io/accelerator/">
          <span itemprop="name"></span>

          <br>

          

          

          
        </span>

        
      </footer>
    </div>

  <script src="../../js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  </body>
</html>

