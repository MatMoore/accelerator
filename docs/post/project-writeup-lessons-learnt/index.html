<!DOCTYPE html>
<html lang="en-gb">

  <head>
    <meta charset="utf-8">
    <meta name="generator" content="Hugo 0.37.1" />

    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="">
    <meta property="og:url" content="https://matmoore.github.io/accelerator/post/project-writeup-lessons-learnt/">

    <title>Building a tool to benchmark search: Results and lessons learnt - Accelerator project journal</title>
    <meta property="og:title" content="Building a tool to benchmark search: Results and lessons learnt - Accelerator project journal">
    <meta property="og:type" content="article">
    <meta name="description" content="">

    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Code+Pro|Arvo:400,700">
    <link rel="stylesheet" href="../../css/highlight.css">
    <link rel="stylesheet" href="../../css/journal.css">
    <link href="../../index.xml" rel="alternate" type="application/rss+xml" title="Accelerator project journal">

  </head>

  <body>
    <div class="container">

      <nav class="site-nav">
        <a href="https://matmoore.github.io/accelerator/">Index</a>
      </nav>

  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">Building a tool to benchmark search: Results and lessons learnt <span class="tag">Week -1</span></h1>
      <time class="post-date" datetime="2018-08-05 13:00:00 BST">05 Aug 2018</time>

    </header>
    

<p>This is the second in a series of blog posts about what I worked on for my data science accelerator project at <a href="https://www.gov.uk/government/organisations/government-digital-service">GDS</a>. It follows <a href="../../post/project-writeup-motivation">part 1: motivation</a>.</p>

<h2 id="what-i-built">What I built</h2>

<p>During the 11? 12? days of the programme, I built a <a href="https://github.com/MatMoore/accelerator">data pipeline</a> that fitted a simplified dynamic bayesian network model to my data about search clicks.</p>

<p>As I alluded to in the previous post, this model allows me to estimate the probability of a
document being relevant to users who entered a certain search term.</p>

<p>I then built a tool that used this model to compare search results to an &ldquo;ideal&rdquo; ranking where everything is ordered according to the model&rsquo;s probabilities. I made this part of <a href="https://github.com/alphagov/search-performance-explorer/pull/62">search performance explorer</a>, which is an app we previously built for evaluating changes to search.</p>

<h2 id="this-is-an-mvp">This is an MVP</h2>

<p>I intentionally kept the tool as simple as possible (just <a href="https://github.com/alphagov/search-performance-explorer/blob/master/lib/health_check/click_model_benchmark.rb">59 lines of ruby code</a>), because I didn&rsquo;t have much time, and this was an experiment. There was no point in adding a bunch of features before I validated whether the tool is useful.</p>

<p>I would have liked to try using the tool to change the search implementation as part of the project, but unfortunately I ran out of time to do this.</p>

<p>The essential requirement for me was to make the output <strong>unambiguous</strong> and <strong>easy to interpret</strong>. The tool we used to use to evaluate search just ran a number of checks, and counted how many passed, so it was very difficult to make decisions based on its output.</p>

<p>For this tool, I made it output a score as a percentage. It looks something like this:</p>

<pre><code class="language-bash">probate: 0.72
tax credits: 0.58
passports: 0.7
mot: 0.66
apprenticeships: 0.84
childcare: 0.49
p45: 0.82
dart charge: 0.69
tax rebate: 0.6
companies house: 0.72
[....]

TOTAL SCORE: 67.6%
</code></pre>

<p>If you can get it to significantly increase the overall score, then you are on to something.</p>

<p>I considered weighting the performance of the various search terms according to usage, but in the end I just kept everything the same, because the search terms I was using weren&rsquo;t representive of user behaviour as a whole.</p>

<h2 id="i-only-looked-at-high-volume-queries">I only looked at high volume queries</h2>

<p>At the beginning of the project I discovered that the data I wanted wasn&rsquo;t being properly exported to BigQuery, so instead of having a years worth of data, I only worked with about a month&rsquo;s worth. This limited what I could do, and I ended up only looking at the highest volume search terms.</p>

<h2 id="the-tool-might-not-work">The tool might not work</h2>

<p>Early on I talked with my mentor about evaluation metrics, and we decided to evaluate the model by measuring the number of sessions where the users &ldquo;chosen&rdquo; document would be ranked higher by the model than under the current implementation of search. But that metric is very conservative as the test users&rsquo; chosen documents are strongly influenced by position bias.</p>

<p>Since I didn&rsquo;t come up with a better metric, I wasn&rsquo;t able to confidently say that using my tool will lead to better search results. Ultimately I won’t know how useful it is until it’s been used in the real world.</p>

<p>I watched a recording of a <a href="https://www.youtube.com/watch?v=gvGfpc7dtMg">lecture by Olivier Chapelle</a> where he talks about the difficulty of evaluating click models and why he doesn&rsquo;t just use the <a href="https://en.wikipedia.org/wiki/Likelihood_function">likelihood</a> when comparing to other models. His method looked specifically at the model&rsquo;s ability to predict clicks when a result is in position 1, which is equivalent to the model&rsquo;s attractiveness parameter. I think this makes sense but it wasn&rsquo;t something I could replicate with my data because GOV.UK search results are very stable and the top result doesn&rsquo;t change often.</p>

<h2 id="the-code-quality-could-be-better">The code quality could be better</h2>

<p>I wrote some horrific code for this project, and it caused me a lot of problems.</p>

<p>I started out with the assumption that pandas was the right tool to use, but user session data didn&rsquo;t really fit a tabular format, so I would have been better off just writing plain python and only using pandas for data exploration.</p>

<p>I ended up storing the cleaned data I was working with in a relational database because it was much easier for me to manipulate that way.</p>

<p>In some cases I got confused by may own names for things. I would use one definition when extracting the data, and then another when using it, because it was hard to remember stuff week to week.</p>

<p>It took me a surprisingly long time to realise that I could save intermediate data between processing steps rather than running everything from scratch every time, but that helped a lot.</p>

<p>I also regret not including human readable identifiers into every dataset I worked with. My data was full of UUIDs like &ldquo;443cf66f-9a51-4bac-9998-b0720cceba7c&rdquo; and whenever I was debugging something I had to work around it by joining in another dataframe.</p>

<h2 id="further-reading">Further reading</h2>

<p>This is going to be the last post on this blog, but if you want to read more about this kind of thing, check out the resources below:</p>

<ul>
<li><a href="https://pdfs.semanticscholar.org/0b19/b37da5e438e6355418c726469f6a00473dc3.pdf">Click models for web search</a> -  a review of different click models</li>
<li><a href="https://github.com/markovi/PyClick">PyClick</a> - a python library of click models</li>
<li><a href="https://youtu.be/gvGfpc7dtMg">A dynamic bayesian network model for web search ranking</a> - a lecture at microsoft research</li>
<li><a href="https://medium.com/@skyetetra/so-your-data-science-project-isnt-working-7bf57e3f12f1">So your data science project isn&rsquo;t working</a> - how to not get discouraged</li>
<li><a href="https://en.wikipedia.org/wiki/Discounted_cumulative_gain#Normalized_DCG">Normalised discounted cumulative gain</a> - the metric I used to calculate the overall score</li>
<li><a href="https://trac.xapian.org/wiki/GSoC2017/LetorClickstream/ProjectPlan">Clickstream mining for learning to rank training data</a> - a google summer of code project that solves the same problem for the Xapian open source search engine</li>
</ul>


  </article>


      <footer class="site-footer">
        <span itemscope itemtype="http://schema.org/Person">
          <link itemprop="url" href="https://matmoore.github.io/accelerator/">
          <span itemprop="name"></span>

          <br>

          

          

          
        </span>

        
      </footer>
    </div>

  <script src="../../js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  </body>
</html>

